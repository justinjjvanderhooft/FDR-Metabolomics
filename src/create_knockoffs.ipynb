{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive spec2vec embeddings of MS/MS spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "#path_data = os.path.join(ROOT, 'data')\n",
    "path_data = 'C:\\\\Users\\\\Gosia\\\\Desktop\\\\'\n",
    "sys.path.insert(0, ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset to create embeddings from, here: pre-processed dataset \"Unique InchiKeys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 files\n",
      "processed 200 files\n",
      "processed 300 files\n",
      "processed 400 files\n",
      "Finished parsing of 457 spectra \n",
      "processed 100 files\n",
      "processed 200 files\n",
      "processed 300 files\n",
      "processed 400 files\n",
      "processed 500 files\n",
      "processed 600 files\n",
      "processed 700 files\n",
      "processed 800 files\n",
      "processed 900 files\n",
      "processed 1000 files\n",
      "processed 1100 files\n",
      "processed 1200 files\n",
      "processed 1300 files\n",
      "processed 1400 files\n",
      "processed 1500 files\n",
      "processed 1600 files\n",
      "processed 1700 files\n",
      "processed 1800 files\n",
      "processed 1900 files\n",
      "processed 2000 files\n",
      "processed 2100 files\n",
      "processed 2200 files\n",
      "processed 2300 files\n",
      "processed 2400 files\n",
      "processed 2500 files\n",
      "processed 2600 files\n",
      "processed 2700 files\n",
      "processed 2800 files\n",
      "processed 2900 files\n",
      "processed 3000 files\n",
      "processed 3100 files\n",
      "processed 3200 files\n",
      "processed 3300 files\n",
      "processed 3400 files\n",
      "processed 3500 files\n",
      "processed 3600 files\n",
      "processed 3700 files\n",
      "processed 3800 files\n",
      "processed 3900 files\n",
      "processed 4000 files\n",
      "Finished parsing of 4095 spectra \n",
      "processed 100\n",
      "processed 200\n",
      "processed 300\n",
      "processed 400\n",
      "processed 100\n",
      "processed 200\n",
      "processed 300\n",
      "processed 400\n",
      "processed 500\n",
      "processed 600\n",
      "processed 700\n",
      "processed 800\n",
      "processed 900\n",
      "processed 1000\n",
      "processed 1100\n",
      "processed 1200\n",
      "processed 1300\n",
      "processed 1400\n",
      "processed 1500\n",
      "processed 1600\n",
      "processed 1700\n",
      "processed 1800\n",
      "processed 1900\n",
      "processed 2000\n",
      "processed 2100\n",
      "processed 2200\n",
      "processed 2300\n",
      "processed 2400\n",
      "processed 2500\n",
      "processed 2600\n",
      "processed 2700\n",
      "processed 2800\n",
      "processed 2900\n",
      "processed 3000\n",
      "processed 3100\n",
      "processed 3200\n",
      "processed 3300\n",
      "processed 3400\n",
      "processed 3500\n",
      "processed 3600\n",
      "processed 3700\n",
      "processed 3800\n",
      "processed 3900\n",
      "processed 4000\n"
     ]
    }
   ],
   "source": [
    "import passatuto_parser as pp\n",
    "sys.path.append('C:\\\\Users\\\\Gosia\\\\Desktop\\\\FDR-Metabolomics\\\\src\\\\passatuto_parser.py')\n",
    "\n",
    "\n",
    "pre_spectrums_query = pp.PassatutoParser(r'C:\\\\Users\\\\Gosia\\\\Desktop\\\\MassbankOrbi').parse_folder()\n",
    "pre_spectrums_lib = pp.PassatutoParser('C:\\\\Users\\\\Gosia\\\\Desktop\\\\Gnps_Noise_Filtered').parse_folder()\n",
    "\n",
    "# Using MatchMS to create spectra for both\n",
    "from matchms.importing.load_from_json import as_spectrum\n",
    "spectrums_query = []\n",
    "for i, s in enumerate( pre_spectrums_query ):\n",
    "    spectrums_query.append(as_spectrum(s))\n",
    "    if i and i % 100 == 0:\n",
    "        print('processed %d' % i)\n",
    "        \n",
    "spectrums_lib = []\n",
    "for i, s in enumerate( pre_spectrums_lib ):\n",
    "    spectrums_lib.append(as_spectrum(s))\n",
    "    if i and i % 100 == 0:\n",
    "        print('processed %d' % i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.importing import load_from_json\n",
    "spectrums_lib = []\n",
    "path_lcms = 'C:\\\\Users\\\\Gosia\\\\Desktop\\\\gnps_from_simon'\n",
    "for s in os.listdir(path_lcms):\n",
    "    spectrums_lib += load_from_json(os.path.join(path_lcms,s))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.filtering import normalize_intensities\n",
    "from matchms.filtering import require_minimum_number_of_peaks\n",
    "from matchms.filtering import select_by_mz\n",
    "from matchms.filtering import select_by_relative_intensity\n",
    "from matchms.filtering import reduce_to_number_of_peaks\n",
    "from matchms.filtering import add_losses\n",
    "def post_process_s2v(s):\n",
    "    \n",
    "    s = normalize_intensities(s)\n",
    "    s = select_by_mz(s, mz_from=0, mz_to=1000)\n",
    "    s = require_minimum_number_of_peaks(s, n_required=10)\n",
    "    s = reduce_to_number_of_peaks(s, n_required=10, ratio_desired=0.5)\n",
    "    if s is None:\n",
    "        return None\n",
    "    s_remove_low_peaks = select_by_relative_intensity(s, intensity_from=0.001)\n",
    "    if len(s_remove_low_peaks.peaks) >= 10:\n",
    "        s = s_remove_low_peaks\n",
    "        \n",
    "    s = add_losses(s, loss_mz_from=5.0, loss_mz_to=200.0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply post processing steps to the data\n",
    "\n",
    "spectrums_lib = [post_process_s2v(s) for s in spectrums_lib]\n",
    "\n",
    "# omit spectrums that didn't qualify for analysis\n",
    "\n",
    "spectrums_lib = [s for s in spectrums_lib if s is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.filtering import normalize_intensities\n",
    "# Spec2Vec trained model requires normalizing\n",
    "spectrums_query = [normalize_intensities(s) for s in spectrums_query]\n",
    "spectrums_lib = [normalize_intensities(s) for s in spectrums_lib]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained spec2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = os.path.join(path_data, \"trained_models\")\n",
    "model_file = os.path.join(path_models, \"spec2vec_AllPositive_ratio05_filtered_201101_iter_15.model\")\n",
    "\n",
    "# Load pretrained model\n",
    "model = gensim.models.Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spectrum \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spec2vec import Spec2Vec\n",
    "from spec2vec import SpectrumDocument\n",
    "\n",
    "#documents_query = [SpectrumDocument(s, n_decimals=2) for s in spectrums_query]\n",
    "documents_lib = [SpectrumDocument(s, n_decimals=2) for s in spectrums_lib]\n",
    "#print(documents_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of size is set from 300 (default) to 100\n",
      "  Epoch 1 of 10.Change in loss after epoch 1: 1903961.625\n",
      "Saving model with name: C:\\Users\\Gosia\\Desktop\\trained_models\\spec2vec_librarymatching_size_100_iter_1.model\n",
      "  Epoch 2 of 10.Change in loss after epoch 2: 1575722.625\n",
      "  Epoch 3 of 10.Change in loss after epoch 3: 1361473.75\n",
      "Saving model with name: C:\\Users\\Gosia\\Desktop\\trained_models\\spec2vec_librarymatching_size_100_iter_3.model\n",
      "  Epoch 4 of 10.Change in loss after epoch 4: 1274022.5\n",
      "  Epoch 5 of 10.Change in loss after epoch 5: 1234125.5\n",
      "Saving model with name: C:\\Users\\Gosia\\Desktop\\trained_models\\spec2vec_librarymatching_size_100_iter_5.model\n",
      "  Epoch 6 of 10.Change in loss after epoch 6: 1168939.0\n",
      "  Epoch 7 of 10.Change in loss after epoch 7: 1046637.0\n",
      "  Epoch 8 of 10.Change in loss after epoch 8: 1020996.0\n",
      "  Epoch 9 of 10.Change in loss after epoch 9: 1000158.0\n",
      "  Epoch 10 of 10.Change in loss after epoch 10: 951152.0\n",
      "Saving model with name: C:\\Users\\Gosia\\Desktop\\trained_models\\spec2vec_librarymatching_size_100.model\n"
     ]
    }
   ],
   "source": [
    "from spec2vec.model_building import train_new_word2vec_model\n",
    "path_models = os.path.join(path_data, \"trained_models\")\n",
    "\n",
    "model_file = os.path.join(path_models, \"spec2vec_librarymatching_size_100.model\")\n",
    "\n",
    "iterations = [1, 3, 5, 10]\n",
    "\n",
    "#Train model with size 10 and default parameters\n",
    "\n",
    "model = train_new_word2vec_model(documents_lib, iterations, model_file, size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Derive embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector size: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66cb076ba5b467485c7b4c213fed197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15039.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # optional, just to get a progress bar\n",
    "from spec2vec.vector_operations import calc_vector\n",
    "\n",
    "\n",
    "intensity_weighting_power = 0.5\n",
    "allowed_missing_percentage = 15 # specify the maximum (weighted) fraction of the spectrum that is allowed to be missing\n",
    "\n",
    "vector_size = model.vector_size\n",
    "print(f\"Embedding vector size: {vector_size}\")\n",
    "\n",
    "#embeddings_spec2vec_query = np.zeros((len(documents_query), vector_size), dtype=\"float\")\n",
    "#for i, doc in enumerate(tqdm(documents_query)):\n",
    "#    embeddings_spec2vec_query[i, 0:vector_size] = calc_vector(model, doc,\n",
    "#                                                        intensity_weighting_power,\n",
    "#                                                        allowed_missing_percentage)\n",
    "embeddings_spec2vec_lib = np.zeros((len(documents_lib), vector_size), dtype=\"float\")\n",
    "for i, doc in enumerate(tqdm(documents_lib)):\n",
    "    embeddings_spec2vec_lib[i, 0:vector_size] = calc_vector(model, doc,\n",
    "                                                        intensity_weighting_power,\n",
    "                                                        allowed_missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-20.2553, 23.3697, 26.3273, -6.411, 59.9514, -8.4595, -18.0223, 12.0185, -33.6996, -29.9557, -19.5061, 25.7586, -13.3538, -34.5308, 53.9283, -19.3537, 7.7151, -6.7035, -37.9293, 5.9646, 1.2801, 58.2668, 38.6722, 55.5007, -6.6895, 18.4156, 20.672, 6.834, 5.4952, -2.1488, 8.936, 23.9831, 15.2567, 23.1338, -73.3666, -26.8928, -1.1262, 15.6832, 9.1843, -20.2924, -1.9166, -29.93, 6.8772, -14.2331, 11.1018, 4.1059, 78.9482, -36.2256, 57.2639, 12.0834, 17.4347, -16.5645, -2.4978, 15.0351, 26.5762, 9.1968, -42.8514, -17.2427, 9.201, -26.8069, -11.5117, -0.2594, 1.751, -8.7163, 0.3586, -20.6555, 52.6081, 39.2642, -0.0907, -39.3073, -21.5782, 30.5351, -15.3938, 17.7743, -1.7567, -49.2557, -20.3511, 29.0435, 1.9563, 61.6495, 25.7956, -1.5794, 57.6926, -21.9423, 8.0085, -6.456, -71.3062, 55.3588, -51.286, 11.3547, 26.7582, -57.767, 42.2818, 90.1023, 65.2442, 25.6384, 63.6896, -67.7287, 36.8816, -26.8009]\n"
     ]
    }
   ],
   "source": [
    "#print([np.round(x, 4) for x in embeddings_spec2vec_query[0,:]])\n",
    "print([np.round(x, 4) for x in embeddings_spec2vec_lib[0,:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating knockoffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from spec2vec import calc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Gaussian mixture model with 3 components, full covariance structure\n",
    "\n",
    "gmm = GMM(n_components=25, covariance_type=\"full\")\n",
    "model = gmm.fit(np.array(embeddings_spec2vec_lib))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Dk matrix in sampling process\n",
    "\n",
    "def find_Dk(covariance_matrix, embedding_dimension):\n",
    "    \n",
    "    eigs = np.linalg.eig(covariance_matrix)[0]\n",
    "    min_eig = min(eigs)\n",
    "    s = min(2*min_eig, 1)\n",
    "    Dk = np.diag([s]*embedding_dimension)\n",
    "    return Dk\n",
    "               \n",
    "def is_pos_semi_def(A, epsilon = 1e-10):    \n",
    "    eigs = np.linalg.eig(A)[0]\n",
    "    min_eig = min(eigs)\n",
    "    return min_eig >= -epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knockoffs(model,vectors):\n",
    "    embedding_dimension = len(vectors[0])\n",
    "    covariances = model.covariances_\n",
    "    means = model.means_\n",
    "    Dks = []\n",
    "    for cov in covariances:\n",
    "        Dk = find_Dk( cov, embedding_dimension )\n",
    "        if not is_pos_semi_def( 2*cov-Dk ):\n",
    "            print('aAAAAAAAAAAAAAAAAaaa')\n",
    "            return\n",
    "        Dks.append(Dk)\n",
    "\n",
    "    knock_means_comps_1 = []\n",
    "    knock_means_comps_2 = []\n",
    "    knock_covs = []\n",
    "    Id = np.diag([1]*embedding_dimension)\n",
    "    for cov,mean,Dk in zip(covariances,means,Dks):\n",
    "        knock_cov = 2*Dk - Dk@(cov@Dk)\n",
    "        knock_mean_comp_1 = Dk@(cov@mean)\n",
    "        knock_mean_comp_2 = Id - Dk@cov\n",
    "        knock_means_comps_1.append(knock_mean_comp_1)\n",
    "        knock_means_comps_2.append(knock_mean_comp_2)\n",
    "        knock_covs.append(knock_cov)\n",
    "        \n",
    "    knockoffs = []\n",
    "    bad_is = []\n",
    "    components = np.arange(len(model.weights_))\n",
    "    probs = model.predict_proba(vectors)\n",
    "    for i, x in enumerate(vectors):        \n",
    "        x_probs = probs[i]\n",
    "        k_posterior = np.random.choice(components, p=x_probs)\n",
    "        knock_mean = knock_means_comps_1[k_posterior] + knock_means_comps_2[k_posterior]@x\n",
    "        knock_cov = knock_covs[k_posterior]\n",
    "        if i and not i%100:\n",
    "            print( 'trying',i )\n",
    "        if not is_pos_semi_def(knock_cov):\n",
    "            bad_is.append(i)\n",
    "            continue\n",
    "        try:\n",
    "            knockoff_sample = np.random.multivariate_normal(knock_mean, knock_cov)\n",
    "        except:\n",
    "            bad_is.append(i)\n",
    "            continue\n",
    "        knockoffs.append(knockoff_sample)\n",
    "        print('success',i)        \n",
    "    return knockoffs, bad_is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-91b430a02389>:40: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  knockoff_sample = np.random.multivariate_normal(knock_mean, knock_cov)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying 100\n",
      "trying 200\n",
      "trying 300\n",
      "trying 400\n",
      "trying 500\n",
      "trying 600\n",
      "trying 700\n",
      "trying 800\n",
      "trying 900\n",
      "trying 1000\n",
      "trying 1100\n",
      "trying 1200\n",
      "trying 1300\n",
      "trying 1400\n",
      "trying 1500\n",
      "trying 1600\n",
      "trying 1700\n",
      "trying 1800\n",
      "trying 1900\n",
      "trying 2000\n",
      "trying 2100\n",
      "trying 2200\n",
      "trying 2300\n",
      "trying 2400\n",
      "trying 2500\n",
      "trying 2600\n",
      "trying 2700\n",
      "trying 2800\n",
      "trying 2900\n",
      "trying 3000\n",
      "trying 3100\n",
      "trying 3200\n",
      "trying 3300\n",
      "trying 3400\n",
      "trying 3500\n",
      "trying 3600\n",
      "trying 3700\n",
      "trying 3800\n",
      "trying 3900\n",
      "trying 4000\n",
      "trying 4100\n",
      "trying 4200\n",
      "trying 4300\n",
      "trying 4400\n",
      "trying 4500\n",
      "trying 4600\n",
      "trying 4700\n",
      "trying 4800\n",
      "trying 4900\n",
      "trying 5000\n",
      "trying 5100\n",
      "trying 5200\n",
      "trying 5300\n",
      "trying 5400\n",
      "trying 5500\n",
      "trying 5600\n",
      "trying 5700\n",
      "trying 5800\n",
      "trying 5900\n",
      "trying 6000\n",
      "trying 6100\n",
      "trying 6200\n",
      "trying 6300\n",
      "trying 6400\n",
      "trying 6500\n",
      "trying 6600\n",
      "trying 6700\n",
      "trying 6800\n",
      "trying 6900\n",
      "trying 7000\n",
      "trying 7100\n",
      "trying 7200\n",
      "trying 7300\n",
      "trying 7400\n",
      "trying 7500\n",
      "trying 7600\n",
      "trying 7700\n",
      "trying 7800\n",
      "trying 7900\n",
      "trying 8000\n",
      "trying 8100\n",
      "trying 8200\n",
      "trying 8300\n",
      "trying 8400\n",
      "trying 8500\n",
      "trying 8600\n",
      "trying 8700\n",
      "trying 8800\n",
      "trying 8900\n",
      "trying 9000\n",
      "trying 9100\n",
      "trying 9200\n",
      "trying 9300\n",
      "trying 9400\n",
      "trying 9500\n",
      "trying 9600\n",
      "trying 9700\n",
      "trying 9800\n",
      "trying 9900\n",
      "trying 10000\n",
      "trying 10100\n",
      "trying 10200\n",
      "trying 10300\n",
      "trying 10400\n",
      "trying 10500\n",
      "trying 10600\n",
      "trying 10700\n",
      "trying 10800\n",
      "trying 10900\n",
      "trying 11000\n",
      "trying 11100\n",
      "trying 11200\n",
      "trying 11300\n",
      "trying 11400\n",
      "trying 11500\n",
      "trying 11600\n",
      "trying 11700\n",
      "trying 11800\n",
      "trying 11900\n",
      "trying 12000\n",
      "trying 12100\n",
      "trying 12200\n",
      "trying 12300\n",
      "trying 12400\n",
      "trying 12500\n",
      "trying 12600\n",
      "trying 12700\n",
      "trying 12800\n",
      "trying 12900\n",
      "trying 13000\n",
      "trying 13100\n",
      "trying 13200\n",
      "trying 13300\n",
      "trying 13400\n",
      "trying 13500\n",
      "trying 13600\n",
      "trying 13700\n",
      "trying 13800\n",
      "trying 13900\n",
      "trying 14000\n",
      "trying 14100\n",
      "trying 14200\n",
      "trying 14300\n",
      "trying 14400\n",
      "trying 14500\n",
      "trying 14600\n",
      "trying 14700\n",
      "trying 14800\n",
      "trying 14900\n",
      "trying 15000\n"
     ]
    }
   ],
   "source": [
    "knockoffs = create_knockoffs(model,np.array(embeddings_spec2vec_lib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
