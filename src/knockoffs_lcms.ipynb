{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of knockoffs from Spec2Vec embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load files with spectra, put your own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matchms.importing import load_from_json\n",
    "from spec2vec import Spec2Vec\n",
    "from spec2vec import SpectrumDocument\n",
    "\n",
    "folder_name = 'C:\\\\Users\\\\Gosia\\\\Desktop'\n",
    "json_file_name = os.path.join(folder_name,'FDR-datsets', 'specs.json')\n",
    "sys.path.append(os.path.join(folder_name,'FDR-Metabolomics', 'src'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrums = load_from_json(json_file_name)\n",
    "spectrums = [s for s in spectrums if s.metadata.get('inchikey')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31147\n",
      "31147\n"
     ]
    }
   ],
   "source": [
    "print(len(spectrums))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping of the inchi key prefixes to the spectrums in order to identify matching spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inchi_dict = {}\n",
    "for s in spectrums:\n",
    "    ik = s.metadata['inchikey']\n",
    "    init_ik = ik.split('-')[0]\n",
    "    if not init_ik in inchi_dict:\n",
    "        inchi_dict[init_ik] = [s]\n",
    "    else:\n",
    "        inchi_dict[init_ik].append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the spectra into library and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the library with matching queries plus noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing queries from multis ensures a possible true match\n",
    "multis = set([i for i,v in inchi_dict.items() if len(v) > 1])\n",
    "\n",
    "matching_keys = np.random.choice(list(multis), size=query_size, replace=False)\n",
    "\n",
    "query_spec = {}\n",
    "spectrums_lib = []\n",
    "# We select query_size queries that have at least 1 matching spectrum in the library\n",
    "for q in matching_keys:\n",
    "    spec_to_add = np.random.choice(inchi_dict[q], size=1, replace=False)\n",
    "    query_spec[spec_to_add[0].metadata['spectrum_id']] = spec_to_add[0]\n",
    "\n",
    "# And everything else goes into the library\n",
    "for s in spectrums:\n",
    "    if s.metadata['spectrum_id'] not in query_spec:\n",
    "        spectrums_lib.append(s)\n",
    "\n",
    "spectrums_query = list(query_spec.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spectrum \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_query = [SpectrumDocument(s, n_decimals=2) for s in spectrums_query]\n",
    "documents_lib = [SpectrumDocument(s, n_decimals=2) for s in spectrums_lib]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_file = os.path.join('C:\\\\Users\\\\Gosia\\\\Desktop\\\\trained_models_1\\\\spec2vec_size_170.model')\n",
    "model = gensim.models.Word2Vec.load(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosine_calc import get_hits\n",
    "\n",
    "allowed_missing_percentage = 15\n",
    "intensity_weighting_power = 0.5\n",
    "\n",
    "hits = get_hits(documents_query, documents_lib, spec2vec_model=model,\n",
    "                intensity_weighting_power=intensity_weighting_power,\n",
    "                allowed_missing_percentage=allowed_missing_percentage, passatutto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating true q-value scores\n",
    "from q_value_calc import calculate_q_value\n",
    "q_list_true = calculate_q_value(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of knockoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector size: 170\n",
      "Embedding vector size: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gosia\\anaconda3\\envs\\fdr-metab\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n",
      "C:\\Users\\Gosia\\anaconda3\\envs\\fdr-metab\\lib\\site-packages\\sklearn\\mixture\\_base.py:265: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn('Initialization %d did not converge. '\n"
     ]
    }
   ],
   "source": [
    "from knockoffs import generate_knockoffs\n",
    "\n",
    "diags = [50]\n",
    "q_list_knockoffs = {}\n",
    "for diag in diags:\n",
    "    for comp in [10]:\n",
    "        try:\n",
    "            knockoff_documents = generate_knockoffs(model,documents_lib,allowed_missing_percentage=allowed_missing_percentage,n_components=comp, diagonal_matrix=diag)\n",
    "            hits_knockoffs = get_hits(documents_query, knockoff_documents, decoys=True, spec2vec_model=model, precursor_tol=3,\n",
    "                                                                    intensity_weighting_power=intensity_weighting_power,\n",
    "                                                                    allowed_missing_percentage=allowed_missing_percentage, passatutto=False)\n",
    "            q_list_knockoffs[(diag, comp)] = calculate_q_value(hits+hits_knockoffs,True)\n",
    "        except Exception as e:\n",
    "            print( diag, \"failed\", e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_q_vals\n",
    "\n",
    "# plot estimated and true q-values\n",
    "def combine_true_est(q_val_true,q_val_est):\n",
    "    res = []\n",
    "    q_idx = 0\n",
    "    for q_e, _, score in q_val_est:\n",
    "        while q_idx < len(q_val_true)-1 and q_val_true[q_idx+1][2] >= score:\n",
    "            q_idx += 1\n",
    "        res.append((score, q_val_true[q_idx][0], q_e))\n",
    "    return res\n",
    "        \n",
    "to_plot = {}\n",
    "for k,v in q_list_knockoffs.items():\n",
    "    to_plot[k] = list(zip(*combine_true_est(q_list_true, v)))[1], list(zip(*combine_true_est(q_list_true, v)))[2]\n",
    "plot_q_vals.plot_q_vals( to_plot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_sorted = sorted(hits, key=lambda h:h.score, reverse=True)\n",
    "for i, h in enumerate( hits_sorted[:20] ):\n",
    "    if not h.hit:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_sorted = sorted(hits, key=lambda h:h.score, reverse=True)\n",
    "for i, h in enumerate( hits_sorted[:20] ):\n",
    "    if not h.hit:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = hits_sorted[5]\n",
    "from pprint import pprint\n",
    "#pprint(hit.query._obj.metadata)\n",
    "pprint([p for p in hit.query._obj.peaks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(hit.target._obj.metadata)\n",
    "pprint([p for p in hit.target._obj.peaks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(q_list_knockoffs.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
